{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HonoursCode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXOZE444ZvOMAME0bnYHgy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/v1umahmo/HonoursProject/blob/main/HonoursCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Necessary Libraries"
      ],
      "metadata": {
        "id": "R0QKRBnAlve7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYtzmrLoimpe"
      },
      "outputs": [],
      "source": [
        "#For data processing\n",
        "import pandas as pd\n",
        "#For linear algebra\n",
        "import numpy as np\n",
        "#Regular expression library\n",
        "import re\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the Training and Testing Data"
      ],
      "metadata": {
        "id": "ZctP3qUgma4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"\") # Import training data from file path\n",
        "test = pd.read_csv(\"\") # Import test data from file path"
      ],
      "metadata": {
        "id": "oBC44jSCloCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Look At The Training Data"
      ],
      "metadata": {
        "id": "aOK6Z59qnY-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.info() # Print training data info"
      ],
      "metadata": {
        "id": "8Kz1l9Wmm0Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10) # Print the first ten tweets from the training data"
      ],
      "metadata": {
        "id": "teVN-tcNm2JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.tail(10) # Print the last ten tweets from the training data"
      ],
      "metadata": {
        "id": "MoSfMz6-nDkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean The Training Data"
      ],
      "metadata": {
        "id": "A3IyTJRHnuvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For loop to go through each tweet for the data cleaning\n",
        "def remove_pattern(input_txt, pattern):\n",
        "  r = re.findall(pattern, input_txt)\n",
        "  for i in r:\n",
        "    input_txt = re.sub(i, ' ', input_txt)\n",
        "  return input_txt"
      ],
      "metadata": {
        "id": "YcTNYuKYnuD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Rid of the user handles\n",
        "train['cleaned_tweets'] = np.vectorise(remove_pattern)(train['tweet'], \"@[\\w]*\")"
      ],
      "metadata": {
        "id": "owC7qV2foeFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10) # Print the first ten tweets from the training data with their twitter handles removed"
      ],
      "metadata": {
        "id": "m7ca3g5DravF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make tweets lower case\n",
        "train['cleaned_tweets'] = train['cleaned_tweets'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "_j7zXb21rsii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10) # Print the first ten tweets from the training data with the tweets lower case"
      ],
      "metadata": {
        "id": "mDW9SSlSsABX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove hashtags and special characters\n",
        "train['cleaned_tweets'] = train['cleaned_tweets'].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))"
      ],
      "metadata": {
        "id": "PN6Yf5gVsCTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "s1hPdNg-yOed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers etc.\n",
        "train['cleaned_tweets'] = train['cleaned_tweets'].apply(lambda x: re.sub(r'[^a-zA-z]',' ',x))"
      ],
      "metadata": {
        "id": "M0RnFKZSyQnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "4iIcM4O1y1kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries for tokenization and stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "bC1gQft55tHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tokenization for the cleaned tweets\n",
        "train['tokenized_tweets'] = train['cleaned_tweets'].apply (lambda x: word_tokenize(x))"
      ],
      "metadata": {
        "id": "qnim9_fhw-r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "XYiECU8zx5xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this to remove words such as if, but, can etc. (Words that have no value)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words"
      ],
      "metadata": {
        "id": "lxfNrJYWx9DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stopwords to the tokenized tweets\n",
        "train['tokenized_tweets_filtered'] = train['tokenized_tweets'].apply(lambda x: [word for word in x if not word in stop_words])"
      ],
      "metadata": {
        "id": "THIsLeWeyqlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "xPY4nDOczRNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import library for stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "2lBDlhq7zZmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stem the tweets and create another colim for tweets that are stemmed\n",
        "train['stemmed_tweets'] = train['tokenized_tweets_filtered'].apply(lambda x: ' '.join([stemming.step(i) for i in x]))"
      ],
      "metadata": {
        "id": "DBOaLThNzxEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "3ivktnOk7htH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import library for lemmatizing\n",
        "from nltk.stem.wordnet import WordnetLemmatizer\n",
        "lemmatizing = WordnetLemmatizer()"
      ],
      "metadata": {
        "id": "NQeBiMdo-Ln2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize the tweets and create another column for tweets that are lemmatized\n",
        "train['lemmatized_tweets'] = train['tokenized_tweets_filtered'].apply(lambda x: ' ' .join([lemmatizing.lemmatize(i) for i in x]))"
      ],
      "metadata": {
        "id": "1mN90JisA_d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "metadata": {
        "id": "r4Budl1lB4gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply Feature Extraction"
      ],
      "metadata": {
        "id": "IwZrFFetpobr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "mdmknkQEpj-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean The Test Data"
      ],
      "metadata": {
        "id": "elPfaJqXALi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test.info()"
      ],
      "metadata": {
        "id": "GgpqvufqAKMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(10)"
      ],
      "metadata": {
        "id": "CpIPuwNwLliI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.tail(10)"
      ],
      "metadata": {
        "id": "j5PToY0qLnFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Rid of the user handles\n",
        "test['cleaned_tweets'] = np.vectorise(remove_pattern)(test['tweet'], \"@[\\w]*\")"
      ],
      "metadata": {
        "id": "FSLz6jwOAPuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(10)"
      ],
      "metadata": {
        "id": "Cur6_WUAAZgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make tweets lower case\n",
        "test['cleaned_tweets'] = test['cleaned_tweets'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "NsDkKQwtAcHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(10)"
      ],
      "metadata": {
        "id": "5ZUJ-sgKAhIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['cleaned_tweets'] = test['cleaned_tweets'].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))"
      ],
      "metadata": {
        "id": "DKgqaTYPCkf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(10)"
      ],
      "metadata": {
        "id": "NAtvkW0hCq68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['cleaned_tweets'] = test['cleaned_tweets'].apply(lambda x: re.sub(r'[^a-zA-z]',' ',x))"
      ],
      "metadata": {
        "id": "BDBHHkT7CsgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(10)"
      ],
      "metadata": {
        "id": "HDNPmXkDC8NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries for tokenization and stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "gH-9sFBUz7zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tokenization for the cleaned tweets\n",
        "test['tokenized_tweets'] = test['cleaned_tweets'].apply (lambda x: word_tokenize(x))"
      ],
      "metadata": {
        "id": "eqYimJDr5zR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(10)"
      ],
      "metadata": {
        "id": "3v1FOaCq58Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words"
      ],
      "metadata": {
        "id": "tl3TnFn56B9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['tokenized_tweets_filtered'] = test['tokenized_tweets'].apply(lambda x: [word for word in x if not word in stop_words])"
      ],
      "metadata": {
        "id": "nmJ4iHC86DIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(10)"
      ],
      "metadata": {
        "id": "cqibJ0pm6P3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import library for stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "R6J8gPWK6Lm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stem the tweets and create another colim for tweets that are stemmed\n",
        "test['stemmed_tweets'] = test['tokenized_tweets_filtered'].apply(lambda x: ' '.join([stemming.step(i) for i in x]))"
      ],
      "metadata": {
        "id": "-D6fLNXU6Sd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(10)"
      ],
      "metadata": {
        "id": "4xAmA7Mh6ekj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import library for lemmatizing\n",
        "from nltk.stem.wordnet import WordnetLemmatizer\n",
        "lemmatizing = WordnetLemmatizer()"
      ],
      "metadata": {
        "id": "sB7rG2uC6Yrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['lemmatized_tweets'] = test['tokenized_tweets_filtered'].apply(lambda x: ' ' .join([lemmatizing.lemmatize(i) for i in x]))"
      ],
      "metadata": {
        "id": "nnli34dA6ngg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head(10)"
      ],
      "metadata": {
        "id": "QujAuZ6j6gT1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}